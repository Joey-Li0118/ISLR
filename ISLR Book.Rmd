---
title: "ISLBook"
output:
  pdf_document: default
  html_document: default
date: "2025-01-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Chapter 2

```{r Chapter 2.3.1}
#ctrl + shift + enter runs the entire chunk 
#ctrl + enter runs the current line

y <- 25325
x <- c(1,2,3,4) #creating a vector
y <- c(5,6,7,8) 
length(x) #lists length of variable 
length(y)
ls() # lists all variables that are saved
rm(x, y) #clears the variables
rm(list = ls()) #clears all variables


matrix <- matrix(c(1,2,3,4), nrow = 2, ncol = 2, byrow = TRUE) #byrow is adding and wrapping matrix[0, :]
print(matrix)

x <- rnorm(50) # creates 50 random vectors with default mean 0 and standard deviation 1
y <- x+ rnorm(50, 50, .1)
cor(x,y)

set.seed(1303) #recreates a set "randomness"
rnorm(50) #now follows a certain output
```

```{r 2.3.2}
jpeg("plot.jpeg") # start a jpg saving process
plot(x, y, xlab = "label for x", ylab = "label for y", main = "title label", col = "purple")
dev.off() #stops the jpg recording process

pdf("plot.pdf") # starts a pdf saving process of all plots
#note requires running the full chunk of the code to save the plot
plot(x, y, xlab = "label for x", ylab = "label for y", main = "title label", col = "blue")
plot(x, y, xlab = "label for x", ylab = "label for y", main = "title label", col = "red")
plot(x, y, xlab = "label for x", ylab = "label for y", main = "title label", col = "green")
dev.off()
dev.list()


x <-seq(-pi, pi, length = 50) #creates 50 evenly spaced 
x[0] #R code is not zero indexed

 y <- x
 f <-outer(x, y, function(x, y) cos(y) / (1 + x^2))
 contour(x, y, f) 
 contour(x, y, f, nlevels = 20, add = T) # creates a contour map with lines labeled, adds to the previous contour
 fa <- (f-t(f)) / 2
 image(x, y, fa) #creates a heat map
 persp(x, y, fa, theta = 60, phi = 0) #creates the 3d model graph, and can change perspectives using theta and phi
 
 #Indexing
 A <- matrix(1:16, 4, 4) 
 A[2,3] #rows, then col
 A[1:3] #rows 1 through 3rd col
 A[1:3, ] #rows 1-3 and all cols
 A[,1:3] #cols 1-3 and all rows
 A[2:3, 1:3] #row 2-3, columns 1-3
 A[c(1, 3), c(2, 4)] # includes 1,2 1,4 3,2 3,4, kind of like distributive property
 A[c(1,3),] #rows 1 and 3
 A[-c(1, 3), ] # rows not 1 or 3
 dim(A)
 library(ISLR2)
 # Auto <-read.table("Auto.data") if this is a file
 View(Auto) # opens up the df page
 head(Auto) # first few rows
 Auto<- na.omit(Auto) #df.dropna
 names(Auto)
 # Auto <-read.table("Auto.data", header = T, na.strings = "?", stringsAsFactors = T)
 #^^^header makes the first line of file the variable names, every string with "?" becomes NA, asFactor turns strings into unique number identifiers kind of like a map key pair(efficient storage, but can't change string)
 plot(Auto$mpg, Auto$weight)
 attach(Auto) #kind of like saying "Using namespace"
 hist(mpg, col = 2)
 cylinders <- as.factor(cylinders)
 plot(cylinders, mpg, col = "red", varwidth = T, horizontal = T) # creates a boxplot when an axis is qualitative
 pairs(Auto)
 pairs(~ mpg + displacement + horsepower + weight + acceleration, data = Auto) #creates these specific pairs
 windows() # creates an interactive window to be able to use the identify fuction and click on points
 plot(horsepower, mpg)
 identify(horsepower, mpg, name)


```

## 2.4 Exercises

### *Conceptual*

1.  For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

    (a) The sample size n is extremely large, and the number of predictors p is small.

        [*Flexible is better. Because the large sample size will help reduce risk of over-fitting and reduce variance, and the small p will minimize effects of high dimensions and create a better, more detailed model*]{style="color: red"}

    (b) The number of predictors p is extremely large, and the number of observations n is small. [*Inflexible is better. Because the small sample size and large p will cause the flexible method to suffer from over-fitting and would try too hard to form patterns from coincidence. Inflexible models are able to reduce noise and risk of over-fitting by assuming a linear relation.*]{style="color: red"}

    (c) The relationship between the predictors and response is highly non-linear. [*Flexible is better. Because an inflexible method would be off unable to capture the non-linearity.*]{style="color: red"}

    (d) The variance of the error terms, i.e. σ2 = Var(ϵ), is extremely high. [*Inflexible is better. Because we aim to reduce the Mean Square Error and if we go with a flexible method, the error would be even higher.*]{style="color: red"}

<!-- -->

2.  Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or pre diction. Finally, provide n and p.

    (a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.

        [*We are not interested in quantitative measurements. We are interested in classification with respect to the factors that affect CEO salaries. This is an inference with n = 500, p = 4.*]{style="color: red"}

    (b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.

        [*This is a classification and prediction with respects to the success/failure of the product with n = 20, p = 14.*]{style="color: red"}

    (c) We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.

        [*This is a regression/prediction problem as we wish to know the %change of the stock market with n = 52, p = 4.*]{style="color: red"}

<!-- -->

3.  We now revisit the bias-variance decomposition.

<!-- -->

(a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.

    ![Taken from internet because drawing is difficult on a computer](images/clipboard-1780759857.png)

(b) Explain why each of the five curves has the shape displayed in part (a).

*Blue: Bias goes down as the flexibility goes up because there are less assumptions being made about the pattern*

*Brown: Variance goes up as the flexibility goes up because you are forming patterns using the exact data points, and so if one point changes, the entire model is thrown off, therefore, this sensitivity to change is the cause of high variance. (Variance in machine learning is a bit different from variance in statistics)*

*Black: Irreducible error*

*Yellow: The training set MSE decreases as model flexibility increases because the model better fits the data and so it will reduce the error of the model.*

*Green: The testing set MSE creates a U shape because of the relations between bias and variance and underfit/overfitting.*

<!-- -->

4.  You will now think of some real-life applications for statistical learning.

<!-- -->

(a) Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.

<!-- -->

1.  Book genres
2.  Dog breeds
3.  Richter Scale

<!-- -->

(b) Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.

<!-- -->

1.  Stock prices
2.  Height prediction
3.  Sports(winning team) betting

<!-- -->

(c) Describe three real-life applications in which cluster analysis might be useful.

<!-- -->

1.  Banks and risk categories
2.  Geographic areas
3.  Customer personas

<!-- -->

5.  What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what Statistical Learning circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?

    [*A flexible approach for regression/classification will make a better prediction for a nonlinear pattern, but it will also require more data and will probably be less explainable/interpretable than the less flexible approach. A less flexible approach would probably be preferred when there is less data to go off of and the relationship is close to linear.*]{style="color: red"}

6.  Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a para metric approach to regression or classification (as opposed to a non parametric approach)? What are its disadvantages? [*A parametric statistical learning approach uses an assumption about the function of the data to make further interpretations and explain patterns while the non-parametric does not and will "flex" to fit the data. Disadvantage is that the error will never be the lowest.*]{style="color: red"}

7.  The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.

```{r}
library(knitr)

# Create the data frame
data <- data.frame(
  Obs = 1:6,
  X1 = c(0, 2, 0, 0, -1, 1),
  X2 = c(3, 0, 1, 1, 0, 1),
  X3 = c(0, 0, 3, 2, 1, 1),
  Y = c("Red", "Red", "Red", "Green", "Green", "Red")
)

# Generate the table
kable(data, caption = "Q7 Table")
```

Suppose we wish to use this data set to make a prediction for Y when X1=X2=X3=0 using K-nearest neighbors.

<!-- -->

(a) Compute the Euclidean distance between each observation and the test point, X1 = X2 = X3 =0.

    [*1,2,3,4,5,6 each respectively pairs with 3, 2, 3.16, 2.24, 1.414, 1.73 #take square root of every x1,x2,x3 - desired x1,x2,x3 and squaring and adding each of the points.*]{style="color: red"}

(b) What is our prediction with K =1? Why?

    k = 1 is green as it was the closest to the desired point (0,0,0)

(c) What is our prediction with K =3? Why?

    k = 3 is 2 red and 1 green, so it should be red. The points near (0,0,0) are 2 red and 1 green.

(d) If the Bayes decision boundary in this problem is highly non linear, then would we expect the best value for K to be large or small? Why?

    Expect k to be small because the boundary has to take very precise and detailed curves which requires the small k.

### *Applied*

8.  This exercise relates to the College data set, which can be found in the file College.csv on the book website. It contains a number of variables for 777 different universities and colleges in the US. The variables are • Private : Public/private indicator • Apps : Number of applications received • Accept : Number of applicants accepted • Enroll : Number of new students enrolled • Top10perc : New students from top 10% of high school class • Top25perc : New students from top 25% of high school class • F.Undergrad : Number of full-time undergraduates • P.Undergrad : Number of part-time undergraduates • Outstate : Out-of-state tuition • Room.Board : Room and board costs • Books : Estimated book costs • Personal : Estimated personal spending • PhD : Percent of faculty with Ph.D.’s • Terminal : Percent of faculty with terminal degree • S.F.Ratio : Student/faculty ratio • perc.alumni : Percent of alumni who donate • Expend : Instructional expenditure per student • Grad.Rate : Graduation rate

Before reading the data into R, it can be viewed in Excel or a text editor. (a) Use the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data.

```{r}
# ctrl + shift + c acts as ctrl + /
# library(ISLR2)
# data(package = "ISLR2") # see all datasets within the package.
# rm(College)
college <- read.csv("C:/Users/joeyl/Downloads/College.csv")
print(college)
```

(b) Look at the data using the View() function. You should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:

```{r}
rownames(college) <- make.names(college[, 1], unique = T) #utilizes the first column of the df as the rownames instead of having it as data
View(college)
rownames(college)
```

You should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. However, we still need to eliminate the first column in the data where the names are stored. Try

```{r}
college <- college[,-1] #all columns except for the first
View(college)
```

Now you should see that the first data column is Private. Note that another column labeled row.names now appears before the Private column. However, this is not a data column but rather the name that R is giving to each row.

(c) 

<!-- -->

i.  Use the summary() function to produce a numerical summary of the variables in the data set.

```{r}
summary(college)
```

<!-- -->

2.  Statistical Learning

<!-- -->

ii. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10].

```{r}
View(college)
college$Private <- ifelse(college$Private == "Yes", 1, 0)
pairs(college[,1:10])
```

iii. Use the plot() function to produce side-by-side boxplots of Outstate versus Private.

```{r}

plot(as.factor(college$Private), college$Outstate, col = "pink", xlab = "Private", ylab = "Outstate", main = "Outstate vs Private")

```

iv. Create a new qualitative variable, called Elite,by binning the Top10 perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%. \> Elite \<-rep("No", nrow(college)) \> Elite[college\$Top10perc \> 50] \<-"Yes" \> Elite \<-as.factor(Elite) \> college \<-data.frame(college, Elite) Use the summary() function to see how many elite univer sities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite.

```{r}
Elite <-rep("No", nrow(college)) #creates a new vector that is initially filled with "No" that's the same number of rows as college
Elite[college$Top10perc > 50] <-"Yes" #if top10percent from highschool exceeds 50% admitted then "Yes", goes by the each row in correspondence
Elite<-as.factor(Elite) #makes it into a factor
college <-data.frame(college, Elite) #adds the vector into the dataframe 

```

v.  Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow = c(2, 2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.

```{r}
par(mfrow = c(2, 2)) #able to graph 4 at once in a 2x2 grid
hist(college$Apps, col = "red")
hist(college$PhD, col = "green")
hist(college$Top10perc, col = "skyblue")
hist(college$Enroll, col = "yellow")
```

vi. Continue exploring the data, and provide a brief summary of what you discover.

<!-- -->

9.  This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.

<!-- -->

(a) Which of the predictors are quantitative, and which are qualitative?

```{r}
library(ISLR2)
sapply(Auto, class) #sapply is a function that can apply the function to all variables
#sapply(Dataframe, Function)

```

[*Qualitative: name* ]{style="color: red"}

[*Quantitative: acceleration, weight, horsepower, displacement, mpg, cylinders, year, origin* ]{style="color: red"}

(b) What is the range of each quantitative predictor? You can answer this using the range() function.
```{r}
quantitative <- Auto[, sapply(Auto, is.numeric)]
range <- sapply(quantitative, range) 
print(range)
```

(c) What is the mean and standard deviation of each quantitative predictor?
```{r}
quantitative <- Auto[, sapply(Auto, is.numeric)]
mean <- sapply(quantitative, mean) 
std <- sapply(quantitative, sd)
print(mean)
print(std)
```

(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?
```{r}
removed <- quantitative[-c(10:85), ]
removedrange <- sapply(removed, range) 
removedmean <- sapply(removed, mean) 
removedstd <- sapply(removed, sd) 
print(removedrange)
print(removedstd)
print(removedstd)
```
(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings. 

```{r}
print(pairs(Auto, col = "blue")) #The variable for the x-axis is the variable represented by the column of the pair plot grid, and the variable for the y-axis is the variable represented by the row of the pair plot grid.

#example mpg(column) is x axis, displacement(row) is y axis

library(GGally)
ggpairs(quantitative, lower = list(continuous = wrap("smooth", color = "blue")), upper = list(continuous = "cor"), diag = list(continuous = wrap("densityDiag", fill = "blue", color = "black")))

```

[*Negative Relationships: mpg/displacement, mpg/horsepower, mpg/cylinders, mpg/weight* ]{style="color: red"}
[*Positive Relationships: mpg/year, cylinders/displacement, cylinders/horsepower, cylinders/weight  *]{style="color: red"}
[*No Relationships/Unable to Form a Relation based on graph: all names, mpg/origin and more....* ]{style="color: red"} 



(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.
[*Variable with strong correlations with mpg are weight, horsepower, cylinders, displacement* ]{style="color: red"} 

<!-- -->

10. This exercise involves the Boston housing data set.

<!-- -->

(a) To begin, load in the Boston data set. The Boston data set is part of the ISLR2 library. \> library(ISLR2) Now the data set is contained in the object Boston. \> Boston Read about the data set: \> ?Boston How many rows are in this data set? How many columns? What do the rows and columns represent?
(b) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.
(c) Are any of the predictors associated with per capita crime rate? If so, explain the relationship.
(d) Doanyofthe census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.
(e) Howmanyofthecensus tracts in this data set bound the Charles river?
(f) What is the median pupil-teacher ratio among the towns in this data set?
(g) Which census tract of Boston has lowest median value of owner occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.
(h) In this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling
